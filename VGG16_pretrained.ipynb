{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Files for Classification"
      ],
      "metadata": {
        "id": "gbxy_E7fYYIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9eU8iP3cR2M",
        "outputId": "3eca4165-f971-47b6-91f9-167359169b68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4vSUqtwPYRHF"
      },
      "outputs": [],
      "source": [
        "!tar -xf /content/drive/MyDrive/Project/ip102_v1.1.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Create the Some folders to the folder to store the images\n",
        "try:\n",
        "  os.mkdir(\"train\")\n",
        "  os.mkdir(\"test\")\n",
        "  os.mkdir(\"val\")\n",
        "except Exception as e:\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "xRPpRkgxYauf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_actual_labels(file_name):\n",
        "    # list to store the names of the image files\n",
        "    file_names = []\n",
        "    # List tostore the actual lables of each image\n",
        "    actual_labels = []\n",
        "\n",
        "    # Read train, test and val file to get the list of files names for each categories\n",
        "    train_file = open(\"ip102_v1.1/\"+file_name)\n",
        "    for l in train_file:\n",
        "        file_names.append(l.split(\" \")[0])\n",
        "        actual_labels.append(int(l.split(\" \")[-1][:-1]))\n",
        "    train_file.close()\n",
        "    return file_names,actual_labels # Return the pair of the list with same size but content different\n",
        "\n",
        "\n",
        "# Creating list to convert the label to actual name of categories\n",
        "super_calss = [\"Rice\", \"Corn\", \"Wheat\", \"Beet\", \"Alfalfa\", \"Vitis\", \"Citrus\", \"Mango\"]\n",
        "super_class_count = [ 14, 13, 9, 8, 13, 16, 19, 10]\n",
        "\n",
        "# Create the subclass in the train, test and val folders to store images\n",
        "for c in super_calss:\n",
        "    try:\n",
        "        os.mkdir(\"train/\"+c)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "    try:\n",
        "        os.mkdir(\"test/\"+c)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "    try:\n",
        "        os.mkdir(\"val/\"+c)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "# This will continue previous process\n",
        "class_cetegories = []\n",
        "prev = 0\n",
        "for i in range(0,len(super_class_count)):\n",
        "    cl = list(range(prev,prev+super_class_count[i]))\n",
        "    class_cetegories.append(cl)\n",
        "    prev = prev + super_class_count[i]\n",
        "\n",
        "    # Define function to map the label number ot the image name\n",
        "def find_category(inp_cat):\n",
        "    for i in range(len(class_cetegories)):\n",
        "        if inp_cat in class_cetegories[i]:\n",
        "            return super_calss[i]\n",
        "        \n",
        "# Function to move files to the respective folder\n",
        "def movefiles(f_type,f_name,f_label):\n",
        "    root_image_folder = \"ip102_v1.1/images/\"\n",
        "    # Following for loop to move images to the trianing folder\n",
        "    for i in range(len(f_name)):\n",
        "        name = f_name[i]\n",
        "        label = f_label[i]\n",
        "        subfolder = find_category(label)  \n",
        "        shutil.move(root_image_folder+name,f_type+\"/\"+subfolder+\"/\"+name)\n",
        "\n",
        "\n",
        "# Create Folders for the sub classification\n",
        "error = \"\"\n",
        "try:\n",
        "    os.mkdir(\"new_train\")\n",
        "    os.mkdir(\"new_train/train\")\n",
        "    os.mkdir(\"new_train/test\")\n",
        "    os.mkdir(\"new_train/val\")\n",
        "    for c in super_calss:\n",
        "        os.mkdir(\"new_train/train/\"+c)\n",
        "        os.mkdir(\"new_train/test/\"+c)\n",
        "        os.mkdir(\"new_train/val/\"+c)\n",
        "except Exception as e:\n",
        "    error = e\n",
        "\n",
        "# File to Read the Actual Class of the image\n",
        "classes_path = \"/content/drive/MyDrive/Project/classes.txt\"\n",
        "dataclass = {}\n",
        "class_file = open(classes_path)\n",
        "for l in class_file:\n",
        "    class_idx = int(l.split()[0])-1\n",
        "    class_name = \" \".join(l.split()[1:])\n",
        "    dataclass[class_idx] = class_name\n",
        "class_file.close()\n",
        "\n",
        "# Following is the function to create the folder, \n",
        "def copy_images_to_sub_categories(r_folder,dict_file):\n",
        "    r_folder = r_folder + \"/\"\n",
        "    super_class_names = os.listdir(r_folder)\n",
        "    \n",
        "    for sc in super_class_names:\n",
        "        for img_n in os.listdir(r_folder+sc):\n",
        "            image_label = dict_file[img_n]\n",
        "            sub_name = dataclass[image_label]\n",
        "            if not os.path.exists(\"new_train/\"+r_folder+\"/\"+sc+\"/\"+sub_name):\n",
        "                os.mkdir(\"new_train/\"+r_folder+\"/\"+sc+\"/\"+sub_name)\n",
        "            shutil.copy(r_folder+sc+\"/\"+img_n,\"new_train/\"+r_folder+\"/\"+sc+\"/\"+sub_name+\"/\"+img_n)\n"
      ],
      "metadata": {
        "id": "A5LfxNKjZARs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move and copy files to respective folders"
      ],
      "metadata": {
        "id": "BJoOILHuZFV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train,l_train = read_actual_labels(\"train.txt\")\n",
        "f_val,l_val = read_actual_labels(\"val.txt\")\n",
        "\n",
        "train_f = {f:l for f,l in zip(f_train,l_train)}\n",
        "val_f = {f:l for f,l in zip(f_val,l_val)}\n",
        "\n",
        "# Move files to the super categories\n",
        "movefiles(\"train\",f_train,l_train)\n",
        "movefiles(\"val\",f_val,l_val)\n",
        "\n",
        "# Move files to the sub categories\n",
        "copy_images_to_sub_categories(\"train\",train_f)\n",
        "copy_images_to_sub_categories(\"val\",val_f)\n"
      ],
      "metadata": {
        "id": "4Lt-YdXZZBQh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import or Install Some libraries"
      ],
      "metadata": {
        "id": "xtVMrSCBbyLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install barbar"
      ],
      "metadata": {
        "id": "UyQ8kLtfZHBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822d2977-8c7c-4895-85d7-564b48681481"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: barbar in /usr/local/lib/python3.9/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing files\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "from distutils.util import strtobool\n",
        "\n",
        "from barbar import Bar\n",
        "import copy\n",
        "import time"
      ],
      "metadata": {
        "id": "HzS-5CgobxED"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(super_category_data,type_m):\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
        "  tc = len(os.listdir(\"new_train/\"+type_m+\"/\"+super_category_data))\n",
        "  model.classifier = nn.Sequential(model.classifier,nn.Linear(1000, tc))\n",
        "  torch.cuda.empty_cache()\n",
        "  model = model.to(\"cuda\")\n",
        "  return model\n",
        "\n",
        "def add_noise(inputs):\n",
        "  noise = torch.randn_like(inputs)*0.2\n",
        "  return inputs + noise\n",
        "\n",
        "# Following is to train the model\n",
        "def train_model(in_model,train_set,valid_set,n_epochs):\n",
        "  weight_decay =  0.00001\n",
        "  val_acc_history = []\n",
        "  train_acc_history = []\n",
        "  val_loss_history = []\n",
        "  train_loss_history = []\n",
        "  dataloaders = {'train': train_set, 'val': valid_set}\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  params_to_update = in_model.parameters()\n",
        "  optimizer_ft = optim.Adam(params_to_update, lr= 0.0001, betas= (0.9, 0.999),\n",
        "                                  eps= 1e-08, weight_decay= weight_decay)\n",
        "  scheduler = optim.lr_scheduler.ExponentialLR(optimizer_ft, gamma= 0.96)\n",
        "  optimizer = optimizer_ft\n",
        "  is_save_checkpoint = False\n",
        "  is_inception = False\n",
        "  ckpepoch = 0\n",
        "  since = time.time()\n",
        "  best_acc = 0.0\n",
        "  num_epochs = n_epochs\n",
        "  device = \"cuda\"\n",
        "  for epoch in range(ckpepoch, num_epochs):\n",
        "    \n",
        "      print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "      print('-' * 10)\n",
        "\n",
        "      for phase in ['train', 'val']:\n",
        "          if phase == 'train':\n",
        "              in_model.train()\n",
        "          else:\n",
        "              in_model.eval()\n",
        "\n",
        "          running_loss = 0.0\n",
        "          running_corrects = 0\n",
        "\n",
        "          for data in Bar(dataloaders[phase]):\n",
        "              if len(data) > 2:\n",
        "                  inputs, _ , labels = data\n",
        "                  inputs = add_noise(inputs)\n",
        "                  inputs = inputs.to(device)\n",
        "              else:\n",
        "                  inputs, labels = data\n",
        "                  inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              with torch.set_grad_enabled(phase == 'train'):\n",
        "                  if is_inception and phase == 'train':\n",
        "                      outputs, aux_outputs = in_model(inputs)\n",
        "                      loss1 = criterion(outputs, labels)\n",
        "                      loss2 = criterion(aux_outputs, labels)\n",
        "                      loss = loss1 + 0.4*loss2\n",
        "                  else:\n",
        "                      outputs = in_model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                  if phase == 'train':\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "              running_loss += loss.item() * inputs.size(0)\n",
        "              inputs.to(\"cpu\")\n",
        "              torch.cuda.empty_cache()\n",
        "              running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "          epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "          epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "          if phase == 'val' and epoch_acc > best_acc:\n",
        "              best_acc = epoch_acc\n",
        "              best_model_wts = copy.deepcopy(in_model.state_dict())\n",
        "              # if is_save_checkpoint:\n",
        "              #     torch.save({'epoch': epoch,\n",
        "              #             'model_state_dict': model.state_dict(),\n",
        "              #             'optimizer_state_dict': optimizer.state_dict(),\n",
        "              #             'best_acc': best_acc,\n",
        "              #             'scheduler_state_dict': scheduler.state_dict(),\n",
        "              #             }, checkpointFn)\n",
        "\n",
        "              count2stop = 0\n",
        "\n",
        "          elif phase == 'val':\n",
        "              count2stop += 1\n",
        "\n",
        "          if phase == 'val':\n",
        "              val_loss_history.append(epoch_loss)\n",
        "              val_acc_history.append(epoch_acc)\n",
        "          if phase == 'train':\n",
        "              train_loss_history.append(epoch_loss)\n",
        "              train_acc_history.append(epoch_acc)\n",
        "\n",
        "\n",
        "      if count2stop == 10:\n",
        "          break\n",
        "\n",
        "      if scheduler:\n",
        "          scheduler.step()\n",
        "          print('lr :', scheduler.get_lr())\n",
        "\n",
        "      print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "  in_model.load_state_dict(best_model_wts)\n",
        "  his = {'train_loss': train_loss_history,\n",
        "        'train_acc': train_acc_history,\n",
        "        'val_loss': val_loss_history,\n",
        "        'val_acc': val_acc_history}"
      ],
      "metadata": {
        "id": "dC9CJBFYcAtL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "weight_decay =  0.00001\n",
        "data_transforms = {\n",
        "            'train': transforms.Compose([\n",
        "               transforms.Resize(256),\n",
        "               transforms.AugMix(),\n",
        "                transforms.RandomCrop(input_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(input_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "        }\n",
        "\n",
        "for s_cname in super_calss:\n",
        "  model_d = get_model(s_cname,\"train\")\n",
        "  batch_size = 32\n",
        "  train_set = ImageFolder(root= \"new_train/train/\"+s_cname, transform= data_transforms['train'])\n",
        "  valid_set = ImageFolder(root= \"new_train/val/\"+s_cname, transform= data_transforms['val'])\n",
        "  train_set = DataLoader(train_set, batch_size= batch_size, shuffle= True, \n",
        "                              num_workers= 8, pin_memory=True)\n",
        "  valid_set = DataLoader(valid_set, batch_size= batch_size, shuffle= False, \n",
        "                                num_workers= 8, pin_memory= True)\n",
        "  train_model(model_d,train_set,valid_set,1)\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.save(model_d.state_dict(), \"/content/drive/MyDrive/Project/\"+s_cname+\"_vgg16\")"
      ],
      "metadata": {
        "id": "YLfu67I7eUKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d60520-0e43-4aa6-d422-a4210b128588"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "5043/5043: [===============================>] - ETA 10.2s\n",
            "train Loss: 2.3094 Acc: 0.2504\n",
            "843/843: [==============================>.] - ETA 3.0s\n",
            "val Loss: 1.9765 Acc: 0.3618\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 2m 41s\n",
            "Best val Acc: 0.361803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:587: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "8404/8404: [===============================>] - ETA 8.2s\n",
            "train Loss: 1.3228 Acc: 0.5806\n",
            "1399/1399: [===============================>] - ETA 2.0s\n",
            "val Loss: 0.9173 Acc: 0.7162\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 4m 12s\n",
            "Best val Acc: 0.716226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "2048/2048: [===============================>] - ETA 8.1s\n",
            "train Loss: 1.9390 Acc: 0.2832\n",
            "340/340: [=============================>..] - ETA 1.8s\n",
            "val Loss: 1.5918 Acc: 0.4265\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 1m 9s\n",
            "Best val Acc: 0.426471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "2649/2649: [===============================>] - ETA 6.5s\n",
            "train Loss: 1.3910 Acc: 0.5092\n",
            "441/441: [=============================>..] - ETA 2.1s\n",
            "val Loss: 1.2187 Acc: 0.5692\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 1m 22s\n",
            "Best val Acc: 0.569161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "6230/6230: [===============================>] - ETA 10.1s\n",
            "train Loss: 1.6626 Acc: 0.4456\n",
            "1037/1037: [===============================>] - ETA 1.5s\n",
            "val Loss: 1.2595 Acc: 0.5805\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 3m 3s\n",
            "Best val Acc: 0.580521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "10525/10525: [===============================>] - ETA 6.1s\n",
            "train Loss: 1.0740 Acc: 0.6780\n",
            "1752/1752: [===============================>] - ETA 3.2s\n",
            "val Loss: 0.7110 Acc: 0.7865\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 5m 8s\n",
            "Best val Acc: 0.786530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "4356/4356: [===============================>] - ETA 7.6s\n",
            "train Loss: 1.9260 Acc: 0.4318\n",
            "725/725: [==============================>.] - ETA 2.3s\n",
            "val Loss: 1.3592 Acc: 0.6055\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 2m 8s\n",
            "Best val Acc: 0.605517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "5840/5840: [===============================>] - ETA 9.0s\n",
            "train Loss: 0.9569 Acc: 0.6998\n",
            "971/971: [==============================>.] - ETA 3.4s\n",
            "val Loss: 0.6102 Acc: 0.7827\n",
            "lr : [9.216e-05]\n",
            "\n",
            "Training complete in 2m 54s\n",
            "Best val Acc: 0.782698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6AQCnSIIJkG"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}